{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T01:29:26.478725Z",
     "start_time": "2018-04-24T01:29:25.930340Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load combine record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T01:29:51.978156Z",
     "start_time": "2018-04-24T01:29:26.478725Z"
    },
    "code_folding": [
     0.0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:186: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\Anaconda2\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:330: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\Anaconda2\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:334: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# %load combine_record.py\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "\n",
    "This is a combined wechat record analysis script file.\n",
    "\n",
    "run order 0.0\n",
    "output: a combined wechat group chattting log\n",
    "\n",
    "\"\"\"\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import chardet\n",
    "import datetime\n",
    "os.chdir(r'D:\\\\0Knowledge\\\\Fudan\\\\0.20170412kidswant\\\\wechat\\\\py_wechat')\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "\"\"\"\n",
    "how to get wechat records:\n",
    "https://www.zhihu.com/question/19924224\n",
    "\n",
    "tables:\n",
    "message \n",
    "  send_msg凡是含有appid的，到appinfo表中进行匹配，例如appid=\\\"\\\"wx992c420c8d64dc18\\\"\\\"\n",
    "          凡是含有title的，<title>少儿手足口病险（秒杀专享）</title>，也取出来\n",
    "  type :1常规文本3应用消息49其他应用分享的信息，可以通过<title>获取47表情10000系统提示信息，加入群聊，撤回消息等\n",
    "    34图片 43图片436207665红包 268435505 应用信息 42 估计是名片 48位置共享 1048625不明，去匹配Appmessage通过msgid\n",
    "    452984881 去匹配appmessage 16777265匹配appmessage 64语音聊天的系统提示 10000进群退群提示 -1879048186去匹配\n",
    "    \n",
    "    基本上就是除了1以外，其他的从appmessage进行匹配\n",
    "rcontact 联系人，个人的表\n",
    "chatroom0830 群聊的基本信息\n",
    "AppMessage 各种应用的提示信息，包括微信自己的红包和第三方的应用分享\n",
    "EmojiInforDesc 表情描述\n",
    "appinfor0830涉及到的应用的描述\n",
    "\n",
    "大概就是这些会比较有用\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "首先是将微信的聊天记录合并起来\n",
    "并对数据做一些初步的处理\n",
    "'''\n",
    "#%%\n",
    "#%%\n",
    "'''\n",
    "#get group display name from the html source code\n",
    "#parse the code with re and extract the displayname\n",
    "#这里得到的是全部的群昵称\n",
    "'''\n",
    "#match chatroom name in Chinese with chatroom id .\n",
    "match_chatroom = pd.read_csv(r'../records/match_chatroom.csv', encoding = 'gb2312')\n",
    "basepath = u'../records/nicknames/'\n",
    "'''\n",
    "input : the directory of html source codes\n",
    "output : a df with group display names\n",
    "'''\n",
    "def parseDisplayname(dir):\n",
    "    displaydf = pd.DataFrame(columns = ['roomname','displayname'])    \n",
    "    for file in os.listdir(dir):\n",
    "        chatroomname = file\n",
    "        with open(basepath + file,'r', encoding = 'utf-8') as f:\n",
    "            htmlfile = f.read()\n",
    "            \n",
    "            #re.complie 将用到的正则表达式预先编码，提高速度，模式re.S，实现跨行匹配\n",
    "            #注意正则表达式里里面的.和()都需要进行转义\n",
    "            re_dis = re.compile(r'getDisplayName\\(currentContact\\.UserName\\)\">(.*?)</p>',re.S)\n",
    "            nicknames = re_dis.findall(htmlfile)\n",
    "            \n",
    "            tmpdf = pd.DataFrame({\n",
    "                    'roomname':chatroomname,\n",
    "                    'displayname':nicknames\n",
    "                    })\n",
    "            displaydf = displaydf.append(tmpdf,ignore_index = True)\n",
    "    return(displaydf)\n",
    "#这里得到的昵称是群昵称，也就是聊天的时候显示的名字\n",
    "#而且，这里的个数只会比chatroom的行数多吧。。。\n",
    "#这里假设没有人退群\n",
    "displaynames = parseDisplayname(basepath)#16356\n",
    "#先假设没有人退群，做merge处理，比对通过roomdata得到的displayname\n",
    "#drop 掉两个群，武进万达孕妈群和辣妈帮\n",
    "displaynames.drop_duplicates(subset = ['displayname','roomid'], inplace = True)\n",
    "displaynames = displaynames.merge(match_chatroom, left_on = 'roomname', right_on = 'name', how = 'inner')#16356\n",
    "\n",
    "#%%\n",
    "#%%\n",
    "'''\n",
    "#read\n",
    "#第一个备份从5月到8月\n",
    "'''\n",
    "message0830 = pd.read_csv(r'../records/170830/message.csv')\n",
    "chatroom0830 = pd.read_csv(r'../records/170830/chatroom.csv')\n",
    "appmsg0830 = pd.read_csv(r'../records/170830/AppMessage.csv')\n",
    "appinfor0830 = pd.read_csv(r'../records/170830/appinfor.csv')\n",
    "#%%\n",
    "\n",
    "#filter chatroom0830\n",
    "#%%\n",
    "nokidswantgroup = [\"7262310752@chatroom\",\"6510569027@chatroom\"]#非孩子王的群\n",
    "chatroom0830 = chatroom0830[~chatroom0830.chatroomname.isin(nokidswantgroup)]\n",
    "remove_room = message0830.talker[message0830.content.str.contains(r'\"移出群聊').fillna(False)]#被5个群移除群聊\n",
    "chatroom0830 = chatroom0830[~chatroom0830.chatroomname.isin(remove_room)]#现在有79个群\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "def match_nickname(chatroomdf):\n",
    "    '''\n",
    "    to get nicknames of members in a group, but the question is when people chat in group, the name displayed in \n",
    "    group chatting is not always the nickname but sometimes the group displayname if they set this name, which can split from the roomdata column.\n",
    "    But! this function is still usable for those chatroom without fault.\n",
    "    \n",
    "    In conclusion, this function get nicknames in the right chatroom\n",
    "    '''\n",
    "    matched_df = pd.DataFrame(columns = ['chatroomname','member','nickname'])\n",
    "    for row in chatroomdf.itertuples():\n",
    "        chatroomname = row[1]\n",
    "        memberset = row[3]\n",
    "        displaynameset = row[4]\n",
    "        memberlist = memberset.split(';')\n",
    "        displaynamelist = displaynameset.split('、')\n",
    "        if len(memberlist) == len(displaynamelist):\n",
    "            for i in range(len(memberlist)):\n",
    "                tmpdf = pd.DataFrame({\n",
    "                        'chatroomname':chatroomname,\n",
    "                        'member':memberlist[i],\n",
    "                        'nickname':displaynamelist[i]                    \n",
    "                        },index = [0])\n",
    "                matched_df = matched_df.append(tmpdf,ignore_index=True)\n",
    "    return(matched_df)\n",
    "    \n",
    "def match_groupdisplayname(chatroomdf,nicknamedf):\n",
    "    '''\n",
    "    to match the member wechat id and group display name in roomdata, which can \n",
    "    not be derived from other columns in chatroom dataframe\n",
    "    '''\n",
    "    matched_df = pd.DataFrame(columns = ['chatroomname','member','displayname'])\n",
    "    for row in chatroomdf.itertuples():\n",
    "        chatroomname = row[1]        \n",
    "        totalset = row[8]\n",
    "        #splitby \\n,but not DC2\\n\n",
    "        tmp1 = re.split('(?<!\\x12)\\n',totalset)\n",
    "        #split by ASCII control codes\n",
    "        tmp2 = [re.split('[\\x00-\\x1F]',x) for x in tmp1]\n",
    "        wechatid = []\n",
    "        displayname = []\n",
    "        tmp3 = []\n",
    "        for i in tmp2:\n",
    "            if len(i) <2:\n",
    "                tmp2.remove(i)\n",
    "            else:\n",
    "                while '' in i:\n",
    "                    i.remove('')\n",
    "                tmp3.append(i)\n",
    "                    \n",
    "                    \n",
    "        for i in tmp3:\n",
    "            if len(i) > 1:\n",
    "                wechatid.append(i[0])\n",
    "                displayname.append(i[1])\n",
    "        \n",
    "        matched_df = matched_df.append(pd.DataFrame({\n",
    "                'chatroomname':chatroomname,\n",
    "                'member':wechatid,\n",
    "                'displayname':displayname\n",
    "                }),ignore_index = True)\n",
    "    #将得到的群昵称和微信昵称进行外链接\n",
    "    matched_df = matched_df.merge(nicknamedf,on = ['chatroomname','member'],how = 'outer')\n",
    "    return(matched_df)\n",
    "        \n",
    "\n",
    "nickname0830 = match_nickname(chatroom0830)\n",
    "nickname0830 = match_groupdisplayname(chatroom0830,nickname0830)\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "'''\n",
    "#get sender and msg\n",
    "#在content中提取sender和发送的内容\n",
    "'''\n",
    "def get_sender_msg(msgdf):\n",
    "    msgdf['sender'] = msgdf.content.str.split(':\\n').str[0]\n",
    "    msgdf['sender'][msgdf.type == 10000] = np.NaN\n",
    "    msgdf['msg'] = msgdf.content.str.split(':\\n',n=1).str[1]\n",
    "#删除msgId列，因为没用,inplace的意思是是否在原始数据上进行操作\n",
    "#删除content列，因为和sender，msg重复了\n",
    "    msgdf.drop(columns = ['msgId','content'], inplace = True)\n",
    "#%%\n",
    "\n",
    "\n",
    "#%%\n",
    "'''\n",
    " #msg filter modigy and combine\n",
    "'''\n",
    "message0830.createTime = pd.to_datetime(message0830.createTime,unit = 'ms')\n",
    "message0830 = message0830[message0830.talker.isin(chatroom0830.chatroomname)]\n",
    "\n",
    "\n",
    "get_sender_msg(message0830)\n",
    "message0830 = message0830.merge(nickname0830,left_on = ['talker','sender'],right_on = ['chatroomname','member'],how = 'left')\n",
    "#所以说total_msg中是将displayname和nickname整合了的\n",
    "total_msg = message0830[['msgSvrId','type','status','isSend','createTime','talker','sender','displayname','nickname','msg']]\n",
    "#%%\n",
    "\n",
    "\n",
    "#%%\n",
    "'''\n",
    "#另一个手机上的备份，从10月到现在\n",
    "#以后的聊天记录备份都可以在该部分处理\n",
    "'''\n",
    "message180119 = pd.read_csv(r'../records/180119/message.csv')\n",
    "chatroom180119 = pd.read_csv(r'../records/180119/chatroom.csv')\n",
    "appmsg180119 = pd.read_csv(r'../records/180119/appmessage.csv')\n",
    "appinfor180119 = pd.read_csv(r'../records/180119/appinfor.csv')\n",
    "#180213是180119的增量更新，可以直接覆盖\n",
    "message180213 = pd.read_csv(r'../records/180213/message.csv')\n",
    "chatroom180213 = pd.read_csv(r'../records/180213/chatroom.csv')\n",
    "appmsg180213 = pd.read_csv(r'../records/180213/appmessage.csv')\n",
    "appinfor180213 = pd.read_csv(r'../records/180213/appinfo.csv')\n",
    "#180310不是上一次的增量更新，需要和之前的合并\n",
    "message180310 = pd.read_csv(r'../records/180310/message.csv')\n",
    "chatroom180310 = pd.read_csv(r'../records/180310/chatroom.csv')\n",
    "appmsg180310 = pd.read_csv(r'../records/180310/appmessage.csv')\n",
    "appinfor180310 = pd.read_csv(r'../records/180310/appinfor.csv')\n",
    "\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "'''\n",
    "chatroom filter \n",
    "'''\n",
    "chatroom180119 = chatroom180119[~chatroom180119.chatroomname.isin(nokidswantgroup)]\n",
    "message180119.talker[message180119.content.str.contains('\"移出群聊').fillna(False)]\n",
    "remove_room = pd.concat([remove_room,message180119.talker[message180119.content.str.contains('\"移出群聊').fillna(False)]],ignore_index = True).drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "chatroom180310 = chatroom180310[~chatroom180310.chatroomname.isin(nokidswantgroup)]\n",
    "message180310.talker[message180310.content.str.contains('\"移出群聊').fillna(False)]#2月份被6个群移除了群聊\n",
    "#和之前的remove_room合并\n",
    "remove_room = pd.concat([remove_room,message180310.talker[message180310.content.str.contains('\"移出群聊').fillna(False)]],ignore_index = True).drop_duplicates().reset_index(drop = True)\n",
    "#这里的remove_room是第一个的子集\n",
    "#所以直接使用\n",
    "chatroom180119 = chatroom180119[~chatroom180119.chatroomname.isin(remove_room)]\n",
    "chatroom180310 = chatroom180310[~chatroom180310.chatroomname.isin(remove_room)]\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "'''\n",
    "get the nickname and group display name\n",
    "'''\n",
    "nickname180119 = match_nickname(chatroom180119)\n",
    "nickname180119 = match_groupdisplayname(chatroom180119,nickname180119)\n",
    "nickname180213 = nickname180119#2月份的记录使用的1月份的chatroom\n",
    "\n",
    "nickname180310 = match_nickname(chatroom180310)\n",
    "nickname180310 = match_groupdisplayname(chatroom180310, nickname180310)\n",
    "#%%\n",
    "\n",
    "\n",
    "#%%\n",
    "'''\n",
    "message filter modify and combine\n",
    "'''\n",
    "message180213 = message180213[message180213.talker.isin(chatroom180119.chatroomname)]#其实此刻有两个群被移除，但是移除的时间并不长，所以不做处理\n",
    "get_sender_msg(message180213)\n",
    "#将display name merge到message上\n",
    "message180213 = message180213.merge(nickname180213,left_on = ['talker','sender'],right_on = ['chatroomname','member'],how = 'left')\n",
    "message180213.createTime = pd.to_datetime(message180213.createTime,unit='ms')\n",
    "total_msg = pd.concat([total_msg,message180213],join='inner',ignore_index = True)\n",
    "\n",
    "message180310 = message180310[message180310.talker.isin(chatroom180310.chatroomname)]\n",
    "get_sender_msg(message180310)\n",
    "#将display name merge到message上\n",
    "message180310 = message180310.merge(nickname180310,left_on = ['talker','sender'],right_on = ['chatroomname','member'],how = 'left')\n",
    "message180310.createTime = pd.to_datetime(message180310.createTime, unit = 'ms')\n",
    "total_msg = pd.concat([total_msg,message180310],join='inner',ignore_index = True)\n",
    "#%%\n",
    "\n",
    "\n",
    "#按照msgSvrId进行去重\n",
    "#这个操作一定要在从模拟器导入数据之前，因为这些数据没有SvrId\n",
    "total_msg.drop_duplicates('msgSvrId',inplace = True)\n",
    "\n",
    "#%%\n",
    "'''\n",
    "#解决聊天中的@问题\n",
    "#这个方法不适合通过软件导出的记录\n",
    "'''\n",
    "#displaynames匹配wechat id\n",
    "tmpdf = pd.DataFrame()\n",
    "for room in displaynames.roomid.unique():\n",
    "    members = chatroom180310[chatroom180310.chatroomname == room].memberlist.tolist()[0].split(';')\n",
    "    members = pd.Series(members)\n",
    "    '''\n",
    "    join : {‘inner’, ‘outer’}, default ‘outer’. How to handle indexes on other axis(es). Outer for union and inner for intersection.\n",
    "ignore_index : boolean, default False. If True, do not use the index values on the concatenation axis. The resulting axis will be labeled 0, ..., n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information.\n",
    "!!! Note the index values on the other axes are still respected in the join.\n",
    "So what i need to do is reindexing the sub displaynams dataframe to 0.1.2... and set join to inner, cause now it is out join and that's awarked\n",
    "    '''\n",
    "    tmp = pd.concat([displaynames[displaynames.roomid == room].reset_index(drop = True),members],axis = 1, ignore_index = True)\n",
    "    tmpdf = pd.concat([tmpdf,tmp],ignore_index = True)\n",
    "    #end of for\n",
    "displaynames = tmpdf.dropna()\n",
    "displaynames.columns = ['displayname','roomname','name','roomid','member']\n",
    "\n",
    "#@人名通过unicode中的\\u2005提取\n",
    "re_unicode = re.compile(u'@(?P<atname>.*?)\\u2005')\n",
    "\n",
    "#msgat = total_msg.msg.str.decode('utf-8').str.extractall(re_unicode)#102397 py2\n",
    "msgat = total_msg.msg.str.extractall(re_unicode)#102397 在py3中文本总是unicode不用担心编码问题，只需要在读取的时候指明编码方式即可\n",
    "msgat.index.levels[0].name = 'msgindex'\n",
    "total_msg.index.name = 'msgindex'\n",
    "# join with index, but the index should be named first\n",
    "msgat = msgat.join(total_msg[['msgSvrId','createTime','talker','sender']])#102397\n",
    "\n",
    "#msgat.atname = msgat.atname.str.encode('utf-8') py3中不再需要\n",
    "msgat = msgat.merge(displaynames,left_on = ['atname','talker'],right_on = ['displayname','roomid'],how = 'left')#102397\n",
    "#只需要member和displayname的对照\n",
    "msgat = msgat[['msgSvrId','atname','createTime','talker','member','sender']]\n",
    "msgat.columns = ['msgSvrId','atname','createTime','talker','member_x','sender']\n",
    "'''\n",
    "在和displaynames表匹配完后，和nicknames表的匹配可以用如下的函数进行\n",
    "分别匹配displaynam和nickname\n",
    "'''\n",
    "def concatNames(atdf,nicknamedf):\n",
    "    #match the displayname\n",
    "    atdf = atdf.merge(nicknamedf,left_on = ['atname','talker'],right_on = ['displayname','chatroomname'],how = 'left')\n",
    "    atdf.member_x[atdf.member_x.isna()] = atdf.member[atdf.member_x.isna()]\n",
    "    atdf = atdf[['msgSvrId','atname','createTime','talker','member_x','sender']]\n",
    "    #match the nickname\n",
    "    atdf = atdf.merge(nicknamedf,left_on = ['atname','talker'],right_on = ['nickname','chatroomname'],how = 'left')\n",
    "    atdf.member_x[atdf.member_x.isna()] = atdf.member[atdf.member_x.isna()]\n",
    "    atdf = atdf[['msgSvrId','atname','createTime','talker','member_x','sender']]\n",
    "    return atdf\n",
    "    \n",
    "msgat = concatNames(msgat,nickname0830)\n",
    "msgat = concatNames(msgat,nickname180119)\n",
    "msgat = concatNames(msgat,nickname180310)\n",
    "\n",
    "len(msgat[msgat.member_x.isna()])*1.0/len(msgat)#有53%的找不到@对应的人\n",
    "#如果找不到displayname对应的wechatid，则删掉\n",
    "msgat.dropna(inplace = True, subset = ['member_x'])\n",
    "nickname0830.dropna(subset = ['displayname']).pipe(lambda x: x.loc[x.displayname.str.contains('赛')])\n",
    "nickname0830.dropna(subset = ['nickname']).pipe(lambda x: x.loc[x.nickname.str.contains('赛')])\n",
    "\n",
    "'''\n",
    "#ToDO\n",
    "用聊天日志中的@名称去匹配displaynames，发现有很多找不到（6701个昵称找不到），试图通过昵称和房间号去nicknames的四个表中去寻找；\n",
    "但是，通过模拟器取得的那些聊天不需要通过这种方法，因为本身记录的就是群昵称，先通过这部分进行算法的训练\n",
    "但是，软件导出的聊天记录也不是全部都有displayname，还是残缺的\n",
    "'''\n",
    "def find_displaynames(df):\n",
    "    #for row in df.itertuples():\n",
    "        #if(row.createTime )\n",
    "    return 0\n",
    "\n",
    "#2017-12-07 04:00:48.001000\n",
    "\n",
    "#%%\n",
    "\n",
    "#从模拟器中导出的聊天记录\n",
    "#from '2017-08-30 12:27:50' to '2017-10-21 19:17:49'\n",
    "#导出的文件含有乱码，这个问题如何解决\n",
    "#%%\n",
    "msg_from_simul = pd.read_csv(r'../records/171021/total_msg(2).csv')\n",
    "msg_from_simul.columns = ['talker','msg','createTime','sender']\n",
    "\n",
    "msg_from_simul['displayname'] = msg_from_simul.sender.str.extract('^(.*?)\\(', expand = False)\n",
    "msg_from_simul.sender = msg_from_simul.sender.str.extract('.*\\\\((.*)\\\\).*', expand = False)\n",
    "msg_from_simul.createTime = pd.to_datetime(msg_from_simul.createTime)\n",
    "#这部分的中文displayname含有乱码，先不使用\n",
    "#total_msg = pd.concat([total_msg,msg_from_simul],join='outer',ignore_index = True)\n",
    "#%%\n",
    "\n",
    "#数据整合工作完成！！！\n",
    "\n",
    "\n",
    "#%%\n",
    "#观察每个月的活跃度\n",
    "#total_msg.groupby(total_msg.createTime.dt.month+total_msg.createTime.dt.year*100).size()\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "#将2018年2月份（含）的聊天记录输出到csv\n",
    "#这里需要注意，最后需要的可能只是所有完整的log，也就是说要使用最新的chatroom进行筛选\n",
    "total_msg[(total_msg.createTime < '2018-03-01')&(total_msg.talker.isin(chatroom180310.chatroomname))].to_pickle(r'../records/sample/sample_msg_pickle')\n",
    "#%%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load classifiertrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T01:54:10.744875Z",
     "start_time": "2018-04-24T01:53:30.086680Z"
    },
    "code_folding": [
     5.0,
     12.0,
     34.0,
     49.0,
     52.0,
     57.0,
     79.0,
     114.0,
     145.0
    ]
   },
   "outputs": [],
   "source": [
    "# %load classifiertrain.py\n",
    "\"\"\"\n",
    "Created on Mon Mar 12 21:47:31 2018\n",
    "\n",
    "@author: Eric\n",
    "\n",
    "run order:1\n",
    "\n",
    "\"\"\"\n",
    "#%%\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import math\n",
    "import timeit\n",
    "#nltk.download()\n",
    "from pyltp import SentenceSplitter\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "'''\n",
    "feature building\n",
    "1.context info of message: author context, conversational context, temperal context\n",
    "2.lexical info of message: discourse(cue words, question, long) content(repeat)\n",
    "\n",
    "We used the development set to approximate the normal densities used in our context models and the evaluation set to obtain\n",
    "the results reported below.\n",
    "'''\n",
    "ann1done = pd.read_csv(r'../records/annotation/anno1done.csv')\n",
    "ann2done = pd.read_csv(r'../records/annotation/anno2done.csv')\n",
    "\n",
    "ann1 = pd.read_csv(r'../records/annotation/anno1.csv')\n",
    "ann2 = pd.read_csv(r'../records/annotation/anno2.csv')\n",
    "\n",
    "ann1 = pd.concat([ann1,ann1done[['thread','appthread']]],axis  = 1)\n",
    "del ann1done\n",
    "ann2 = pd.concat([ann2,ann2done[['thread','appthread']]],axis  = 1)\n",
    "del ann2done\n",
    "\n",
    "ann1.createTime = pd.to_datetime(ann1.createTime)\n",
    "ann2.createTime = pd.to_datetime(ann2.createTime)\n",
    "\n",
    "ann1.dropna(subset = ['msg','thread'],how = 'any', inplace = True)\n",
    "ann2.dropna(subset = ['msg','thread'],how = 'any', inplace = True)\n",
    "\n",
    "ann1 = ann1[ann1.thread > -2]\n",
    "ann2 = ann2[ann2.thread > -2]\n",
    "#set the time window 群组讨论的当天和前后半天\n",
    "twindow = 0.5#这个应该用在全局的totalmsg表上，每次处理响应窗口的数据\n",
    "#%%\n",
    "#%%\n",
    "'''\n",
    "the author context of a message m, denoted by CA(m), is the set of other messages\n",
    "written by m’s author am:\n",
    "'''\n",
    "#context info of message\n",
    "#author context\n",
    "    \n",
    "totalTimediff = []\n",
    "for row in ann1.itertuples():\n",
    "    msgid = row[2]\n",
    "    sender = row[6]\n",
    "    msgdate = row[4]\n",
    "    maxdate = msgdate + datetime.timedelta(days = 1)\n",
    "    mindate = msgdate - datetime.timedelta(days = 1)\n",
    "    threadid = row[10] \n",
    "    autherContext = ann1[(ann1.sender == sender)&(ann1.msgSvrId != msgid)]\n",
    "    timediff = autherContext[autherContext.thread == threadid]['createTime'] - msgdate\n",
    "    timediff = timediff.tolist()#单位秒\n",
    "    timediff = [x.total_seconds()/60 for x in timediff]\n",
    "    totalTimediff.extend(timediff)\n",
    "\n",
    "#估计这个timediff的参数\n",
    "plt.hist(totalTimediff, normed = True)\n",
    "st.norm.fit(totalTimediff)#loc = 0 scale = 58.7\n",
    "#%%\n",
    "#%%\n",
    "'''\n",
    "conversational context \n",
    "'''\n",
    "totalTimediff = []\n",
    "for row in ann1.itertuples():\n",
    "    msgid = row[2]\n",
    "    sender = row[6]\n",
    "    msgdate = row[4]\n",
    "    maxdate = msgdate + datetime.timedelta(days = 1)\n",
    "    mindate = msgdate - datetime.timedelta(days = 1)\n",
    "    threadid = row[10]\n",
    "    #我提到了那些人\n",
    "    mentionNames = msgat[(msgat.msgSvrId == msgid)&(msgat.createTime > mindate)&(msgat.createTime < maxdate)]['member_x'].tolist()\n",
    "    #我被那些人提到\n",
    "    mentionNames.extend(msgat[(msgat.member_x == sender)&(msgat.createTime > mindate)&(msgat.createTime < maxdate)]['sender'].tolist())\n",
    "    #这些人说的话\n",
    "    converContext = ann1[ann1.sender.isin(mentionNames)&(ann1.createTime > mindate)&(ann1.createTime < maxdate)]\n",
    "    timediff = converContext[converContext.thread == threadid]['createTime'] - msgdate\n",
    "    timediff = timediff.tolist()\n",
    "    totalTimediff.extend([x.total_seconds()/60 for x in timediff])\n",
    "    \n",
    "#估计这个timediff的参数\n",
    "plt.hist(totalTimediff, normed = True)\n",
    "#减去均值，使其为0\n",
    "avg = sum(totalTimediff) / float(len(totalTimediff))\n",
    "totalTimediff = [x - avg for x in totalTimediff]\n",
    "st.norm.fit(totalTimediff)#loc = 0 scale = 9.6\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "'''\n",
    "temporal context\n",
    "\n",
    "'''\n",
    "totalTimediff = []\n",
    "for row in ann1.itertuples():\n",
    "    msgid = row[2]\n",
    "    sender = row[6]\n",
    "    msgdate = row[4]\n",
    "    maxdate = msgdate + datetime.timedelta(days = 1)\n",
    "    mindate = msgdate - datetime.timedelta(days = 1)\n",
    "    threadid = row[10]\n",
    "    \n",
    "    temporalContext = ann1[(ann1.createTime > mindate)&(ann1.createTime < maxdate)&(ann1.msgSvrId != msgid)]\n",
    "    timediff = temporalContext[temporalContext.thread == threadid]['createTime'] - msgdate\n",
    "    timediff = timediff.tolist()\n",
    "    totalTimediff.extend([x.total_seconds()/60 for x in timediff])\n",
    "    \n",
    "#估计这个timediff的参数\n",
    "plt.hist(totalTimediff, normed = True)\n",
    "#减去均值，使其为0\n",
    "avg = sum(totalTimediff) / float(len(totalTimediff))\n",
    "totalTimediff = [x - avg for x in totalTimediff]\n",
    "st.norm.fit(totalTimediff)#loc = 0 scale = 117\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T01:54:10.744875Z",
     "start_time": "2018-04-24T01:53:30.086680Z"
    },
    "code_folding": [
     5.0,
     12.0,
     34.0,
     49.0,
     52.0,
     57.0,
     79.0,
     114.0,
     145.0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\envs\\py3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n",
      "D:\\Anaconda2\\envs\\py3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "'''\n",
    "compute the context-free freatures.\n",
    "represent sentences with vectors of terms counts.\n",
    "Terms:\n",
    "    1.bag of words\n",
    "    2.length ?\n",
    "'''\n",
    "#get bag of words\n",
    "#split sentences\n",
    "'''\n",
    "#问题：是否对msg做一些过滤，字段\n",
    "1.通过停止词\n",
    "2.是否过滤掉@微信名这种\n",
    "3.过滤掉只出现过一次的词\n",
    "'''\n",
    "re_filter = re.compile(u'@(?P<atname>.*?)\\u2005')\n",
    "ann1['msg'] = ann1.msg.str.replace(re_filter,'')#replace the @wechatid with '', this should be done when deal with predict dataset.\n",
    "ann2['msg'] = ann2.msg.str.replace(re_filter,'')\n",
    "\n",
    "import os\n",
    "LTP_DATA_DIR = r'D://ltp_data//ltp_data_v3.4.0'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "#在此基础上加入分词的语句\n",
    "from pyltp import Segmentor\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "wordsLists = []\n",
    "\n",
    "for index, msg in ann2['msg'].iteritems():\n",
    "    #print(msg)\n",
    "    sents = SentenceSplitter.split(msg)\n",
    "    #sentsLists.append(sents)\n",
    "    wordslist = []\n",
    "    for sent in sents:\n",
    "        #过滤句子中的标点符号\n",
    "        words = segmentor.segment(sent)\n",
    "        wordslist.extend(list(words))\n",
    "    wordsLists.append(wordslist)\n",
    "ann2['splitwords'] = wordsLists\n",
    "\n",
    "segmentor.release()  # 释放模型,分词完毕\n",
    "'''\n",
    "#the filted msg may contain nothing, cause everything is filted\n",
    "#remove common words\n",
    "with open(r'stopwords.tab', encoding = 'utf-8') as f_stop:\n",
    "     stopwords = [line.strip() for line in f_stop]\n",
    "     \n",
    "ann2['splitwords'] = [[word for word in wordslist if word not in stopwords] for wordslist in ann2['splitwords'].tolist()]\n",
    "'''\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "'''\n",
    "BUILD THE BAG OF WORDS\n",
    "'''\n",
    "from gensim import corpora, models\n",
    "from six import iteritems\n",
    "dictionary = corpora.Dictionary(ann2.splitwords.tolist())\n",
    "#read stopwords list\n",
    "with open(r'stopwords.tab', encoding = 'utf-8') as f_stop:\n",
    "     stopwords = [line.strip() for line in f_stop]\n",
    "#remove stop words and words appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stopwords if stopword in dictionary.token2id]\n",
    "#alternative stop_ids\n",
    "#dictionary.filter_n_most_frequent(50)\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "dictionary.compactify()# remove gaps in id sequence after words that were removed\n",
    "\n",
    "#initialize the bag of words model\n",
    "corpus = [dictionary.doc2bow(text) for text in ann2['splitwords'].tolist()]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf =  tfidf[corpus]\n",
    "#model persistency\n",
    "#dictionary.save('ann2_split_words.dict')#store the dictionary for future reference\n",
    "#corpus_tfidf.save('corpus_tfidf')\n",
    "'''\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)\n",
    "'''\n",
    "ann2['tfidf'] = list(corpus_tfidf)\n",
    "#convert from list of tuple to list of dicts\n",
    "ann2['tfidf'] = list(map(lambda x: dict(x), ann2['tfidf']))\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "'''\n",
    "BY NOW THE TFIDF MODEL IS THE CONTEXT FREE REPRESENTATION OF MESSAGES.\n",
    "NOW I WILL EXPAND THIS REPRESENTATION WITH CONTEXT INFO WITH PRAMETERS TRAINED BEFORE\n",
    "HOW TO EXPAND: FOR EACH MESSAGE i BUILD THE FOLLOWING FUNCTIONS\n",
    "    AUTHOR CONTEXT PROB autherProb(msgi)\n",
    "    CONVERSATIONAL CONTEXT PROB converProb(msgi)\n",
    "    TEMPORAL CONTEXT PROB\n",
    "    BOTH OF THESE FUNCTIONS RESTRICT THE TIMEWINDOW BETWEEN [DATE - 1DAY,DATE + 1DAY](to achieve this, build a df by date)\n",
    "    \n",
    "'''\n",
    "def probMultiTfidf(probArray, tfidfList):\n",
    "    '''Multiply the context tfidf with probability in the same thread.\n",
    "    Args:\n",
    "        probArray(Array): probability in the same thread of msg context\n",
    "        tfidfList(List): the tfidfList of msg context\n",
    "    Returns:\n",
    "        The return tuple\n",
    "    '''    \n",
    "    '''\n",
    "    rList = []\n",
    "    rappend = rList.append\n",
    "    for i in range(len(probArray)):\n",
    "        r2List = []\n",
    "        r2append = r2List.append\n",
    "        #at the very beginning, the tfidfList is a list of tuple\n",
    "        #fixed this to dict\n",
    "        for term, freq in tfidfList[i].items():\n",
    "            #r2List.append((term, freq*probArray[i]))\n",
    "            r2append((term, freq*probArray[i]))\n",
    "        #rList.append(r2List)\n",
    "        rappend(r2List)\n",
    "    return(rList)\n",
    "    '''\n",
    "    #alternative\n",
    "    return([{k: v for k,v in zip(tfidfList[i].keys(),probArray[i] * np.array(list(tfidfList[i].values())))} for i in range(len(probArray))])\n",
    "def sumDicts(dict1,dict2):\n",
    "    '''Sum dict1 with dict2, only 2.\n",
    "    \n",
    "    '''\n",
    "    return({k:dict1.get(k,0) + dict2.get(k,0) for k in dict1.keys()|dict2.keys()})    \n",
    "def sumTfidf(tfidfList):\n",
    "    '''Sum the new computed tfidf\n",
    "    Args:\n",
    "        tfidfList: A list of tfidf\n",
    "    Return:\n",
    "        the return list: expanded tfidf dict\n",
    "    '''\n",
    "    rList = {}\n",
    "    for tfidf in tfidfList:\n",
    "        '''\n",
    "        for term, freq in tfidf:\n",
    "            if(term in rList):#has_key was removed in Python3\n",
    "                rList[term] = rList[term] + freq\n",
    "            else:\n",
    "                rList[term] = freq\n",
    "        '''\n",
    "        #alternative\n",
    "        rList = sumDicts(tfidf,rList)\n",
    "    return(rList)\n",
    "def autherProb(row, t_scale, contextdf, w_auther):\n",
    "    '''Expand the context-free info of msgi with auther context.\n",
    "    \n",
    "    Args:\n",
    "        row(Series): One raw of msgdf.itertuples() to be expanded\n",
    "        t_scale(float): The scale of normal distribution trained before\n",
    "        contextdf(dataframe): The dataframe of msgs between [DATE - 1DAY,DATE + 1DAY]\n",
    "        w_auther(float): Weight for auther context.\n",
    "    Returns:\n",
    "        The return vector. The expand vector repretation of msgi with info from its auther context.\n",
    "    '''\n",
    "    msgid = row[2]\n",
    "    sender = row[6]\n",
    "    msgdate = row[4]\n",
    "    #maxdate = msgdate + datetime.timedelta(days = 1)\n",
    "    #mindate = msgdate - datetime.timedelta(days = 1)\n",
    "    #threadid = row[10] \n",
    "    #based on the defination, the autherContext dont have to be in the same thread, just the same auther.\n",
    "    #FIXED\n",
    "    autherContext = contextdf[(contextdf.sender == sender)&(contextdf.msgSvrId != msgid)]\n",
    "    timediff = autherContext['createTime'] - msgdate\n",
    "    timediff = timediff.tolist()#in seconds\n",
    "    timediff = [x.total_seconds()/60for x in timediff]\n",
    "    tfidfList = autherContext['tfidf'].tolist()\n",
    "    probArray = st.norm.pdf(timediff, scale = t_scale)\n",
    "    newTfidf = probMultiTfidf(probArray,tfidfList)\n",
    "\n",
    "    #return(sumTfidf(newTfidf))\n",
    "    return({k:(w_auther*v)  for k,v in sumTfidf(newTfidf).items()})\n",
    "\n",
    "def converProb(row, t_scale, contextdf, msgat, w_conver):\n",
    "    '''Expand the context-free info of msgi with conversational context.\n",
    "    \n",
    "    Args:\n",
    "        row(Series): One raw of msgdf.itertuples() to be expanded\n",
    "        t_scale(float): The scale of normal distribution trained before\n",
    "        contextdf(dataframe): The dataframe of msgs between [DATE - 1DAY,DATE + 1DAY]\n",
    "        msgat(dataframe): The dataframe of msgat between [DATE - 1DAY,DATE + 1DAY]\n",
    "                        , for the reduction of repeted searching the bounded date of msgat\n",
    "        w_conver(float): weight for conversational context\n",
    "    Returns:\n",
    "        The return vector. The expand vector repretation of msgi with info from its conversational context.    \n",
    "    '''\n",
    "    msgid = row[2]\n",
    "    sender = row[6]\n",
    "    msgdate = row[4]\n",
    "    #maxdate = msgdate + datetime.timedelta(days = 1)\n",
    "    #mindate = msgdate - datetime.timedelta(days = 1)\n",
    "    #threadid = row[10] \n",
    "    #autherContext = contextdf[(contextdf.sender == sender)&(contextdf.msgSvrId != msgid)&(contextdf.thread == threadid)]\n",
    "    #我提到了那些人\n",
    "    mentionNames = msgat[msgat.msgSvrId == msgid]['member_x'].tolist()\n",
    "    #我被那些人提到\n",
    "    mentionNames.extend(msgat[msgat.member_x == sender]['sender'].tolist())\n",
    "    #这些人说的话\n",
    "    converContext = contextdf[(contextdf.sender.isin(mentionNames))]\n",
    "    \n",
    "    timediff = converContext['createTime'] - msgdate\n",
    "    timediff = timediff.tolist()#in seconds\n",
    "    timediff = [x.total_seconds()/60for x in timediff]\n",
    "    tfidfList = converContext['tfidf'].tolist()\n",
    "    probArray = st.norm.pdf(timediff, scale = t_scale)\n",
    "    newTfidf = probMultiTfidf(probArray,tfidfList)\n",
    "\n",
    "    return({k:(v*w_conver) for k,v in sumTfidf(newTfidf).items()})    \n",
    "def tempProb(row, t_scale, contextdf, w_temp):\n",
    "    '''Expand the context-free info of msgi with temporal context.\n",
    "    \n",
    "    Args:\n",
    "        row(Series): One raw of msgdf.itertuples() to be expanded\n",
    "        t_scale(float): The scale of normal distribution trained before\n",
    "        contextdf(dataframe): The dataframe of msgs between [DATE - 1DAY,DATE + 1DAY]\n",
    "\n",
    "    Returns:\n",
    "        The return vector. The expand vector repretation of msgi with info from its temporal context. \n",
    "    \n",
    "    '''\n",
    "    msgid = row[2]\n",
    "    #sender = row[6]\n",
    "    msgdate = row[4]\n",
    "\n",
    "    #threadid = row[10] \n",
    "    tempContext = contextdf[(contextdf.msgSvrId != msgid)]\n",
    "    timediff = tempContext['createTime'] - msgdate\n",
    "    timediff = timediff.tolist()#in seconds\n",
    "    timediff = [x.total_seconds()/60for x in timediff]\n",
    "    tfidfList = tempContext['tfidf'].tolist()\n",
    "    probArray = st.norm.pdf(timediff, scale = t_scale)\n",
    "    newTfidf = probMultiTfidf(probArray,tfidfList)\n",
    "\n",
    "    return({k:(v*w_temp)for k,v in sumTfidf(newTfidf).items()})\n",
    "\n",
    "def expandedMsg(contextFree, autherContext, converContext, tempContext, w_contextFree):\n",
    "    '''Sum the context-free msg tfidf with all context info.\n",
    "    Args:\n",
    "        contextFree(List): The tfidf list of the raw msg, each element is a dict\n",
    "        autherContext(List): The tfidf List of expanded auther context info.\n",
    "        tempContext(List): The tfidf List of expanded temporal context info.\n",
    "        converContext(List): The tfidf List of expanded conversational context info.\n",
    "        w_contextFree(float): Weights for context-free\n",
    "    Returns:\n",
    "        The return vector of expanded representation.\n",
    "    '''\n",
    "    rList = []\n",
    "    for i in range(len(contextFree)):\n",
    "        #rList.append(sumDicts(sumDicts(sumDicts(contextFree[i],autherContext[i]),converContext[i]),tempContext[i]))\n",
    "        contextPart ={k:(v*(1-w_contextFree))for k,v in sumDicts(sumDicts(autherContext[i],converContext[i]),tempContext[i]).items()}\n",
    "        contextFreePart = {k:(v*w_contextFree) for k,v in contextFree[i].items()}\n",
    "        rList.append(sumDicts(contextFreePart, contextPart))\n",
    "    return(rList)\n",
    "auther_scale = 58.7\n",
    "conver_scale = 9.6\n",
    "temporal_scale = 117\n",
    "\n",
    "w_contextFree = 0.45\n",
    "w_auther = 0.3\n",
    "w_conver = 0.6\n",
    "w_temp = 1-w_auther-w_conver\n",
    "#weights = {'w_contextFree':w_contextFree,'w_context':w_context,'w_auther':w_auther,'w_conver':w_conver,'w_temp':w_temp}\n",
    "#access the date property with a .dt accessor\n",
    "autherExpandList = []\n",
    "converExpandList = []\n",
    "tempExpandList = []\n",
    "for date in ann2['createTime'].dt.date.unique():\n",
    "    #FIEXED\n",
    "    maxdate = datetime.datetime(date.year,date.month,date.day) + datetime.timedelta(days = twindow+1)\n",
    "    mindate = datetime.datetime(date.year,date.month,date.day) - datetime.timedelta(days = twindow)\n",
    "    #slice the msgat dataframe with bounded time period\n",
    "    msgatdf = msgat[(msgat.createTime > mindate)&(msgat.createTime < maxdate)]\n",
    "    contextdf = ann2[(ann2.createTime > mindate)&(ann2.createTime < maxdate) ]\n",
    "    targetdf = ann2[ann2.createTime.dt.date == date] \n",
    "    for row in targetdf.itertuples():\n",
    "        autherExpandList.append(autherProb(row, auther_scale, contextdf, w_auther))\n",
    "        converExpandList.append(converProb(row, conver_scale, contextdf, msgatdf, w_conver))\n",
    "        tempExpandList.append(tempProb(row, temporal_scale, contextdf, w_temp))\n",
    "        #break\n",
    "\n",
    "ann2['extended'] = expandedMsg(ann2['tfidf'].tolist(),autherExpandList,converExpandList,tempExpandList, w_contextFree)\n",
    "\n",
    "#ann2.to_csv('../records/annotation/anno2extented.csv')\n",
    "#%%\n",
    "\n",
    "#%%\n",
    "'''Compute the similarity between a given msg and existing threads\n",
    "Single pass clustering is performed.(reference about single pass tech: \n",
    "    http://facweb.cs.depaul.edu/mobasher/classes/csc575/assignments/single-pass.html)\n",
    "1.Treat the first msg as a single-message cluster T.\n",
    "2.for each remaining msg m compute for All the existing threads.\n",
    "    sim(m, T)= max sim(m, mi) mi belongs to All the existing threads.\n",
    "    sim(m, mi) = cosin similarity between these two msgs.\n",
    "'''\n",
    "def dictNorm(dict1):\n",
    "    '''Compute the norm of a dictionary just like a vector\n",
    "    Args:\n",
    "        dict1(dict): the square of sum of each value in this dict.\n",
    "    Returns:\n",
    "        The return float.\n",
    "    '''    \n",
    "    return(sum([v*v for v in dict1.values()]))\n",
    "def dotProduct(dict1,dict2):\n",
    "    '''Compute the dot product of two dicts, like vectors\n",
    "    Args:\n",
    "        dict1, dict2(dicts): two tfidf dicts.\n",
    "    Returns:\n",
    "        The return float.\n",
    "    '''\n",
    "    return(sum([dict1[k]*dict2[k] for k in dict1.keys()&dict2.keys()]))\n",
    "\n",
    "def similarity(msgdf, targetmsgid, msgdate, threadDict, threshold):\n",
    "    '''Pairwise similarity function.\n",
    "    \n",
    "    1. Turn the date into date counts.\n",
    "    2. The composation of Theadid:\n",
    "        Part1: Date count * 10e4\n",
    "        Part2: increamental thread id counts.(suppose this count will not exceed 10e3)\n",
    "        Theadid = Part1 + Part2\n",
    "    3. Only compute in the range of [Date - 1, Date]\n",
    "    Args:\n",
    "        msgdf(dataframe): The dataframe of msg log\n",
    "        targetmsgid(String): The target unique msg id to be compared.\n",
    "        msgdate(int): The date count from the beginning date.\n",
    "        threadDict(dict): key= thread id, value = List of msgid.\n",
    "        threshold(float): if the max similarity is under threshold, start a new cluster containing only this msg\n",
    "    Returns:\n",
    "        The updated threadDict.\n",
    "    '''\n",
    "    if(len(threadDict) == 0):\n",
    "        threadid = 1 + 10000*msgdate\n",
    "        threadDict[threadid] = [targetmsgid]\n",
    "    else:\n",
    "        max_similarity_thread = 0\n",
    "        max_similarity = 0\n",
    "        #find the max similarity and the corresponding thread over all threads.\n",
    "        for thread, msgids in threadDict.items():\n",
    "            #Notice, the threadDict may be NULL!\n",
    "            threaddate = thread // 10000\n",
    "            if(threaddate < msgdate - 1): continue\n",
    "            for msgid in msgids:\n",
    "                targetmsg = msgdf[msgdf.msgSvrId == targetmsgid]['extended'].item()#get the exact dict rather than dict and object type\n",
    "                comparedmsg = msgdf[msgdf.msgSvrId == msgid]['extended'].item()\n",
    "                cosine = dotProduct(targetmsg,comparedmsg)/math.sqrt(dictNorm(targetmsg)*dictNorm(comparedmsg))\n",
    "                if (cosine > max_similarity):\n",
    "                    max_similarity = cosine\n",
    "                    max_similarity_thread = thread\n",
    "                     \n",
    "        if(max_similarity > threshold):\n",
    "            #print(max_similarity)\n",
    "            threadDict[max_similarity_thread].append(targetmsgid)\n",
    "        else:\n",
    "            #create a new thread\n",
    "            if(threaddate == msgdate):\n",
    "                threadDict[max(threadDict.keys())%10000 + 1 + msgdate*10000] = [targetmsgid]\n",
    "            if(threaddate < msgdate):\n",
    "                #if this a new date and new thread, then reset the thread id to 1.\n",
    "                threadDict[1 + msgdate*10000] = [targetmsgid]\n",
    "    return(threadDict)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect the threads.\n",
    "AND\n",
    "Test the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T01:58:10.127145Z",
     "start_time": "2018-04-24T01:54:58.171690Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "# cell magic starts with %% and line magic starts with %\n",
    "threadDict = {}       \n",
    "begin_date = min(ann2['createTime'])\n",
    "threshold = 0.7\n",
    "# Use count to control how many msgs to distengle thread.\n",
    "count = 1\n",
    "for row in ann2.itertuples():\n",
    "    msgid = row[2]\n",
    "    msgdate = row[4]\n",
    "    msgdate = (msgdate - begin_date).days + 1\n",
    "    threadDict = similarity(ann2, msgid, msgdate, threadDict, threshold)\n",
    "    #should not use multiprocessing, cause the thread detection is in the time sequence.\n",
    "    \n",
    "    #count += 1\n",
    "    #if(count >50): break\n",
    "\n",
    "#TODO\n",
    "# compare the performance between for loop map() and list comprehension    \n",
    "# map is not suitable for this iteration.\n",
    "# and list comprehension is not suitable too.\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T18:02:12.785547Z",
     "start_time": "2018-04-22T18:02:12.764491Z"
    }
   },
   "outputs": [],
   "source": [
    "#for k,v in threadDict.items():\n",
    "#    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T01:58:48.685756Z",
     "start_time": "2018-04-24T01:58:48.648684Z"
    },
    "code_folding": [
     1.0
    ]
   },
   "outputs": [],
   "source": [
    "realThreadDict = {}\n",
    "for thread in ann2['thread'].unique():\n",
    "    msgs = ann2[ann2['thread'] == thread]['msgSvrId'].tolist()\n",
    "    realThreadDict[thread] = msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T01:58:54.739938Z",
     "start_time": "2018-04-24T01:58:54.729912Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "'''Choose the F value to be the object and try to train the optimal model.\n",
    "1. Build the F value.\n",
    "2. Adjust the parameters to maximize F value.\n",
    "''' \n",
    "def recall(ti,tj):\n",
    "    '''The recall between the real thread i and the detected thread j.\n",
    "    \n",
    "    Recall(i,j) = nij / ni\n",
    "    where nij is the number of msgs of the real thread i in the detected thread j.\n",
    "    ni is the number of msgs in the real thread i.\n",
    "\n",
    "    Args:\n",
    "        ti(int): The real thread number, also the key in the realThreadDict.\n",
    "        tj(int): The detected thread number, also the key in the threadDict.\n",
    "    Return:\n",
    "        The return float number.    \n",
    "    '''\n",
    "    realMsg = realThreadDict[ti]\n",
    "    detectMsg = threadDict[tj]\n",
    "    nij = len([real for real in realMsg if real in detectMsg])\n",
    "    ni = len(realMsg)\n",
    "    #check the result with jupyter console\n",
    "    #good result\n",
    "    return(nij/ni)\n",
    "def precision(ti,tj):\n",
    "    '''Precision(i,j) = nij / nj\n",
    "    where nj is the number of msgs in the detected thread j.\n",
    "    \n",
    "    Args:\n",
    "        ti(int): The real thread number, also the key in the realThreadDict.\n",
    "        tj(int): The detected thread number, also the key in the threadDict.\n",
    "    Return:\n",
    "        The return float number.        \n",
    "    '''\n",
    "    realMsg = realThreadDict[ti]\n",
    "    detectMsg = threadDict[tj]\n",
    "    nij = len([real for real in realMsg if real in detectMsg])\n",
    "    nj = len(detectMsg)\n",
    "    #check the result with jupyter console\n",
    "    #good result\n",
    "    return(nij/nj)\n",
    "def pairf(ti,tj):\n",
    "    ''' F(i,j) = 2*Precision*Recall /(Precision + Recall)\n",
    "    is the F measure of detected thread j and the real thread i.\n",
    "    '''  \n",
    "    prevalue = precision(ti,tj)\n",
    "    revalue = recall(ti,tj)\n",
    "    #print(prevalue, revalue)\n",
    "    if((prevalue ==0 )|( revalue == 0)): \n",
    "        return(0)\n",
    "    else:\n",
    "        return(2*prevalue*revalue/(prevalue+revalue))\n",
    "\n",
    "def fvalue(realThreadDict, threadDict):\n",
    "    '''The whole F measure of the detection result in a stream\n",
    "is defined as a weighted sum over all threads as follow.\n",
    "    Args:\n",
    "        realThreadDict(dict): The dict of ground true threads of msgs. Key is thread id and value is msgid list.\n",
    "        threadDict(dict): The dict of detected thread.\n",
    "    '''\n",
    "    max_pairf, wholef = 0, 0\n",
    "    len_msg = len(ann2)\n",
    "    for realThread in realThreadDict:\n",
    "        for detectThread in threadDict:\n",
    "            value_pairf = pairf(realThread, detectThread)\n",
    "            if(value_pairf > max_pairf): max_pairf = value_pairf\n",
    "        wholef = wholef + len(realThreadDict[realThread])*value_pairf\n",
    "\n",
    "    return wholef/len_msg\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T01:59:08.606019Z",
     "start_time": "2018-04-24T01:59:08.529816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00261519302615193"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fvalue(realThreadDict, threadDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T02:08:28.992164Z",
     "start_time": "2018-04-24T02:08:28.982166Z"
    }
   },
   "outputs": [],
   "source": [
    "def tuning(w_contextFree = 0.45,w_auther = 0.3, w_conver = 0.6, w_temp = 0.1, threshold = 0.7):\n",
    "    '''Tuning parameters.\n",
    "    1. Compute the extended msg tfidf with all the parameters.\n",
    "    2. Cluster msgs into threads.\n",
    "    3. Compute the F value.\n",
    "\n",
    "    Args:\n",
    "        w_*, threshold(float): all the weights and threshold to be tuning.\n",
    "\n",
    "    ''' \n",
    "    #access the date property with a .dt accessor\n",
    "    autherExpandList = []\n",
    "    converExpandList = []\n",
    "    tempExpandList = []\n",
    "    for date in ann2['createTime'].dt.date.unique():\n",
    "        #FIEXED\n",
    "        maxdate = datetime.datetime(date.year,date.month,date.day) + datetime.timedelta(days = twindow+1)\n",
    "        mindate = datetime.datetime(date.year,date.month,date.day) - datetime.timedelta(days = twindow)\n",
    "        #slice the msgat dataframe with bounded time period\n",
    "        msgatdf = msgat[(msgat.createTime > mindate)&(msgat.createTime < maxdate)]\n",
    "        contextdf = ann2[(ann2.createTime > mindate)&(ann2.createTime < maxdate) ]\n",
    "        targetdf = ann2[ann2.createTime.dt.date == date] \n",
    "        for row in targetdf.itertuples():\n",
    "            autherExpandList.append(autherProb(row, auther_scale, contextdf, w_auther))\n",
    "            converExpandList.append(converProb(row, conver_scale, contextdf, msgatdf, w_conver))\n",
    "            tempExpandList.append(tempProb(row, temporal_scale, contextdf, w_temp))\n",
    "    \n",
    "    #ann2['tfidf'] = list(map(lambda x: dict(x), ann2['tfidf']))\n",
    "    #ann2['extended'] = expandedMsg(ann2['tfidf'].tolist(),autherExpandList,converExpandList,tempExpandList, w_contextFree)\n",
    "    #extendedtfidf is a list of dicts.\n",
    "    extendedtfidf = expandedMsg(ann2['tfidf'].tolist(),autherExpandList,converExpandList,tempExpandList, w_contextFree)\n",
    "    \n",
    "    #Then compute the similarity and get the thread.\n",
    "    threadDict = {}       \n",
    "    begin_date = min(ann2['createTime'])\n",
    "    for row in ann2.itertuples():\n",
    "        msgid = row[2]\n",
    "        msgdate = row[4]\n",
    "        msgdate = (msgdate - begin_date).days + 1\n",
    "        threadDict = similarity(ann2, msgid, msgdate, threadDict, threshold)\n",
    "    \n",
    "    #Then compute the F value.\n",
    "    return(fvalue(realThreadDict, threadDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T17:57:47.998420Z",
     "start_time": "2018-04-22T17:57:19.119Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#search best parameters\n",
    "contextFreeList = [i/10 for i in range(1,3,step = 0.5)]\n",
    "#%debug\n",
    "for i in contextFreeList:\n",
    "    print(i,tuning(w_contextFree = i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-24T02:51:38.291Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%debug\n",
    "tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "造成contextfree参数变化但是F不变的原因在于 context free的tfidf和 extended 的tfidf相比大了太多（e-1/e-5）\n",
    ",所以应该直接使用term freq进行扩充吗？\n",
    "不是，更主要的原因出现在auther_Prob等的概率上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T19:50:15.698651Z",
     "start_time": "2018-04-23T19:50:15.691634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.4817966406077289,\n",
       " 1: 0.222270827298042,\n",
       " 2: 0.3228899823130633,\n",
       " 3: 0.3425837264331833,\n",
       " 4: 0.3589708703355047,\n",
       " 5: 0.3685892530799719,\n",
       " 6: 0.4817966406077289}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann2['tfidf'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T02:13:47.808435Z",
     "start_time": "2018-04-24T02:13:47.798410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.21691880859509494,\n",
       " 1: 0.1019265818802901,\n",
       " 2: 0.14583174092960416,\n",
       " 3: 0.1576901578720162,\n",
       " 4: 0.16302535459193637,\n",
       " 5: 0.16737925162080566,\n",
       " 6: 0.21717417975746264,\n",
       " 7: 0.0008171642775682974,\n",
       " 8: 0.00159157647203461,\n",
       " 9: 0.002115964978626841,\n",
       " 10: 0.0002821783650919936,\n",
       " 11: 0.0002149099542170135,\n",
       " 12: 0.0035055325968951423,\n",
       " 13: 0.0017829703883001935,\n",
       " 14: 0.0020136067722110675,\n",
       " 15: 0.0016161010311329892,\n",
       " 16: 0.000252772827285321,\n",
       " 17: 0.0006418371073635242,\n",
       " 18: 0.00362947804448041,\n",
       " 19: 0.0015054808310182974,\n",
       " 20: 0.00021154123907689152,\n",
       " 21: 0.0013726418386152551,\n",
       " 22: 6.382745798036741e-05,\n",
       " 23: 0.0003616348498169077,\n",
       " 24: 0.0006404030843569339,\n",
       " 25: 0.0019271209435752419,\n",
       " 26: 0.00169940924902967,\n",
       " 27: 0.0004410466303800156,\n",
       " 28: 0.001588785861273323,\n",
       " 29: 0.00028272745443085095,\n",
       " 30: 0.00010559023683213355,\n",
       " 31: 0.0002513514442865208,\n",
       " 32: 8.919141671928322e-05,\n",
       " 33: 0.0010169269733844638,\n",
       " 34: 0.00027481035959706795,\n",
       " 35: 0.00040290201657068977,\n",
       " 36: 0.0020536640025606992,\n",
       " 37: 0.0005834894984821776,\n",
       " 38: 0.0024810822959687746,\n",
       " 39: 0.001153365192943374,\n",
       " 40: 0.0012658353916009145,\n",
       " 41: 0.002775081048008229,\n",
       " 42: 1.326109421301295e-05,\n",
       " 43: 3.063954661393718e-05,\n",
       " 44: 0.00011871110399481274,\n",
       " 45: 0.0014371410023812758,\n",
       " 46: 0.0001522534665838014,\n",
       " 47: 0.0001292715036414628,\n",
       " 48: 0.00019715191342894675,\n",
       " 49: 0.0002452936701435752,\n",
       " 50: 0.00018923411681907954,\n",
       " 51: 1.7378452400924236e-05,\n",
       " 52: 0.000900921085598169,\n",
       " 53: 1.7378452400924236e-05,\n",
       " 54: 3.475690480184847e-05,\n",
       " 55: 2.9730526964658085e-05,\n",
       " 56: 0.0001275845642929391,\n",
       " 57: 0.00012339204856240375,\n",
       " 58: 1.2349737510265073e-05,\n",
       " 59: 0.00012199575161461927,\n",
       " 60: 0.0020900616302011433,\n",
       " 61: 0.0013969521839155713,\n",
       " 62: 0.0021310321967715354,\n",
       " 63: 0.00019229728445590232,\n",
       " 64: 0.0011849049326591572,\n",
       " 65: 0.00133341655989381,\n",
       " 66: 0.0006471383467998179,\n",
       " 67: 0.0007037174873261377,\n",
       " 68: 0.00018594634476088505,\n",
       " 69: 0.0004202908048139218,\n",
       " 70: 0.00012112979797860818,\n",
       " 71: 0.0023337991155024307,\n",
       " 72: 5.235310561615062e-05,\n",
       " 73: 0.0005328666064414526,\n",
       " 74: 0.00041792818131535225,\n",
       " 75: 0.0008661907878531316,\n",
       " 76: 0.00020821393095329943,\n",
       " 77: 0.000149301798217416,\n",
       " 78: 0.0006951181485814154,\n",
       " 79: 0.0004477602151665939,\n",
       " 80: 0.0004043958711679705,\n",
       " 81: 0.0008410954786360357,\n",
       " 82: 0.0015477550016810384,\n",
       " 83: 0.0003653081689143169,\n",
       " 84: 0.002261974152748873,\n",
       " 85: 0.00038829990026498023,\n",
       " 86: 0.00032272836007117084,\n",
       " 87: 0.0011715173941099204,\n",
       " 88: 0.0010230947494817267,\n",
       " 89: 0.00036590461836684055,\n",
       " 90: 0.0005723130028679591,\n",
       " 91: 0.0005763639430821095,\n",
       " 92: 0.00020819236539385946,\n",
       " 93: 0.0002063135659083128,\n",
       " 94: 0.0003584384467018653,\n",
       " 95: 0.0001967333243632847,\n",
       " 96: 0.000532867298687668,\n",
       " 97: 0.0015473064758842474,\n",
       " 98: 0.0002279163434099993,\n",
       " 99: 0.00035560623337832546,\n",
       " 100: 0.00011661912664507103,\n",
       " 101: 0.00011661912664507103,\n",
       " 102: 0.0002916584815264662,\n",
       " 103: 0.0004681574223911688,\n",
       " 104: 0.0012440857323478478,\n",
       " 105: 0.00022377137690899842,\n",
       " 106: 0.00021369676406375552,\n",
       " 107: 0.00015619145789458436,\n",
       " 108: 0.0004158798896901145,\n",
       " 109: 0.00022230897255181076,\n",
       " 110: 0.0002687784276119341,\n",
       " 111: 0.00031794713461776074,\n",
       " 112: 0.0005893507146148796,\n",
       " 113: 9.26167626446715e-05,\n",
       " 114: 9.26167626446715e-05,\n",
       " 115: 0.0002606559475544035,\n",
       " 116: 0.0002860462892066449,\n",
       " 117: 0.001032071276184099,\n",
       " 118: 0.00018812349960222168,\n",
       " 119: 0.000296290216815729,\n",
       " 120: 0.0005299509527235084,\n",
       " 121: 0.0006137939556818805,\n",
       " 122: 0.0008444240575639722,\n",
       " 123: 0.00015061912653254497,\n",
       " 124: 0.00022674564679567527,\n",
       " 125: 0.00018750295946071087,\n",
       " 126: 0.00131886385011246,\n",
       " 127: 0.0003441272947739276,\n",
       " 128: 0.0006328166197187849,\n",
       " 129: 0.00034060001201615585,\n",
       " 130: 0.00028914232098673545,\n",
       " 131: 0.0003976688062246273,\n",
       " 132: 0.00010270106098863552,\n",
       " 133: 0.00010270106098863552,\n",
       " 134: 0.00024243152430639345,\n",
       " 135: 0.000257256137936063,\n",
       " 136: 0.0002200863261745045,\n",
       " 137: 0.0002493852576381389,\n",
       " 138: 0.0004727110954414368,\n",
       " 139: 0.00048330300700902886,\n",
       " 140: 0.00020110649352130132,\n",
       " 141: 0.00021560634502103593,\n",
       " 142: 0.0002706495438746113,\n",
       " 143: 0.00023996300427907028,\n",
       " 144: 0.00021192409036209213,\n",
       " 145: 0.0001287277153797574,\n",
       " 146: 0.00012786760042288542,\n",
       " 147: 0.0002578055125455845,\n",
       " 148: 0.0002578055125455845,\n",
       " 149: 0.00018750118179627822,\n",
       " 150: 0.0001497429116147447,\n",
       " 151: 0.0007684410372987912,\n",
       " 152: 0.0009036595735098341,\n",
       " 153: 0.0004044754594444916,\n",
       " 154: 0.0005155661007268436,\n",
       " 155: 0.0001388544570362267,\n",
       " 156: 8.450234025179977e-05,\n",
       " 157: 8.450234025179977e-05,\n",
       " 158: 0.00014488184912286818,\n",
       " 159: 0.00020103447690962803,\n",
       " 160: 0.00018749903720984335,\n",
       " 161: 0.00023851900810134782,\n",
       " 162: 7.67805034945546e-05,\n",
       " 163: 8.465347720399952e-05,\n",
       " 164: 0.0001306662348600441,\n",
       " 165: 0.00037658110206509717,\n",
       " 166: 0.0003895669824240093,\n",
       " 167: 0.00010796002657666302,\n",
       " 168: 0.00012442945932830817,\n",
       " 169: 0.00026166515210008194,\n",
       " 170: 0.0003492076929338658,\n",
       " 171: 0.0003744126158142674,\n",
       " 172: 0.00013111267478786796,\n",
       " 173: 8.176941828118284e-05,\n",
       " 174: 0.0003677024455241126,\n",
       " 175: 0.00032847451320855165,\n",
       " 176: 0.00025069465350858214,\n",
       " 177: 0.00013257087480731072,\n",
       " 178: 0.00017358505551186187,\n",
       " 179: 0.00016089769100856654,\n",
       " 180: 0.0002333671877765923,\n",
       " 181: 7.903535824305262e-05,\n",
       " 182: 7.360372474947531e-05,\n",
       " 183: 7.360372474947531e-05,\n",
       " 184: 0.0003545703880160375,\n",
       " 185: 0.000122574745032142,\n",
       " 186: 0.0003196582963639286,\n",
       " 187: 0.0003196007065571506,\n",
       " 188: 0.000550022304646293,\n",
       " 189: 0.00023187881245559825,\n",
       " 190: 0.0001808897011112652,\n",
       " 191: 0.00020993543915825825,\n",
       " 192: 0.0006710857502300861,\n",
       " 193: 0.0002701565352113724,\n",
       " 194: 0.0002585785476616428,\n",
       " 195: 0.00027307379948643125,\n",
       " 196: 0.0003127977689797976,\n",
       " 197: 0.0002292327341335703,\n",
       " 198: 0.00042333479427203343,\n",
       " 199: 0.00031577224827732846,\n",
       " 200: 9.73635728338772e-05,\n",
       " 201: 0.00020226103273556035,\n",
       " 202: 0.00018697671392009152,\n",
       " 203: 9.771245336758635e-05,\n",
       " 204: 0.00018697578638332817,\n",
       " 205: 0.000422084503868747,\n",
       " 206: 0.000236382387495771,\n",
       " 207: 0.00018919100234021785,\n",
       " 208: 0.00016031557375916308,\n",
       " 209: 0.00018696591447675166,\n",
       " 210: 0.0005112448841433489,\n",
       " 211: 8.530450221072908e-05,\n",
       " 212: 0.0001144923613312443,\n",
       " 213: 0.00026965962604157183,\n",
       " 214: 0.00026071851030621363,\n",
       " 215: 1.6469432751645138e-05,\n",
       " 216: 0.00016881755040555718,\n",
       " 217: 0.00015482015286557812,\n",
       " 218: 8.234716375822569e-06,\n",
       " 219: 1.53375858853482e-05,\n",
       " 220: 1.53375858853482e-05,\n",
       " 221: 0.00023500079485898502,\n",
       " 222: 4.1173581879112844e-06,\n",
       " 223: 1.2352074563733854e-05,\n",
       " 224: 8.234716375822569e-06,\n",
       " 225: 0.00010106608650343618,\n",
       " 226: 0.00024414942536783485,\n",
       " 227: 9.469923832195956e-05,\n",
       " 228: 9.749247799084772e-05,\n",
       " 229: 4.1173581879112844e-06,\n",
       " 230: 1.2352074563733854e-05,\n",
       " 231: 4.1173581879112844e-06,\n",
       " 232: 1.2352074563733854e-05,\n",
       " 233: 0.0001044974371869471,\n",
       " 234: 4.1173581879112844e-06,\n",
       " 235: 1.6469432751645138e-05,\n",
       " 236: 3.63363193840744e-06,\n",
       " 237: 0.00031530137899053235,\n",
       " 238: 0.00018694354847716523,\n",
       " 239: 0.00011062353460489032,\n",
       " 240: 0.00018931408340852008,\n",
       " 241: 0.0004555124049836847,\n",
       " 242: 0.0009248822344624283,\n",
       " 243: 0.0003281584699794804,\n",
       " 244: 0.000197047404021585,\n",
       " 245: 0.0012209483093126462,\n",
       " 246: 0.00169645728386084,\n",
       " 247: 0.0003640328176321007,\n",
       " 248: 0.000277297539154618,\n",
       " 249: 0.00021354989207181943,\n",
       " 250: 0.00021414992051752794,\n",
       " 251: 0.00019145551764075731,\n",
       " 252: 9.337511980293644e-05,\n",
       " 253: 9.337511980293644e-05,\n",
       " 254: 0.0012496982736869635,\n",
       " 255: 0.00018255846930136753,\n",
       " 256: 0.0010246080296290544,\n",
       " 257: 7.358541932834468e-05,\n",
       " 258: 9.8763467344779e-05,\n",
       " 259: 0.0001272003298839563,\n",
       " 260: 0.00021679682466036037,\n",
       " 261: 0.00023958014660015314,\n",
       " 262: 0.00014276167289281627,\n",
       " 263: 0.0002911807934771194,\n",
       " 264: 0.0003176459890840153,\n",
       " 265: 0.00023936669565579637,\n",
       " 266: 0.00029360645034166047,\n",
       " 267: 0.0002911836220330292,\n",
       " 268: 9.684791491908548e-05,\n",
       " 269: 0.00026051813643804625,\n",
       " 270: 8.888868423814088e-05,\n",
       " 271: 0.00011638282786578947,\n",
       " 272: 0.00023346103375144643,\n",
       " 273: 0.00022910074312288998,\n",
       " 274: 0.00028251501837620025,\n",
       " 275: 5.997437495525033e-05,\n",
       " 276: 9.340798327759733e-05,\n",
       " 277: 0.00026680807696303864,\n",
       " 278: 0.00024372808086230475,\n",
       " 279: 0.0001049588406135672,\n",
       " 280: 0.00024050055923005397,\n",
       " 281: 0.0006287602723054271,\n",
       " 282: 0.00023983146141350712,\n",
       " 283: 8.884400156383636e-05,\n",
       " 284: 0.0002855782388567013,\n",
       " 285: 0.0002356538148241447,\n",
       " 286: 0.00035639212866941656,\n",
       " 287: 0.0009483667435363515,\n",
       " 288: 0.0003106666236161772,\n",
       " 289: 0.00012101941273567738,\n",
       " 290: 0.0003182756204183713,\n",
       " 291: 8.401943748361781e-05,\n",
       " 292: 0.000153515623669012,\n",
       " 293: 0.00011499004957630389,\n",
       " 294: 0.00018682240889874216,\n",
       " 295: 7.630840310770215e-05,\n",
       " 296: 7.630840310770215e-05,\n",
       " 297: 0.00020929632273891384,\n",
       " 298: 0.00021192019685084456,\n",
       " 299: 0.0002894790936488039,\n",
       " 300: 0.0002083907715307175,\n",
       " 301: 0.00010878220783943768,\n",
       " 302: 9.960856369127981e-05,\n",
       " 303: 0.0003429651229280925,\n",
       " 304: 0.00012773303802342625,\n",
       " 305: 0.0001558864536512716,\n",
       " 306: 0.00013848405640787912,\n",
       " 307: 0.00042061496836354103,\n",
       " 308: 8.530256561095027e-05,\n",
       " 309: 8.530256561095027e-05,\n",
       " 310: 0.00012561051302915715,\n",
       " 311: 0.00011000582887419967,\n",
       " 312: 5.992751933172679e-05,\n",
       " 313: 0.0001794583176357479,\n",
       " 314: 9.271456031208499e-05,\n",
       " 315: 9.271456031208499e-05,\n",
       " 316: 0.0001431858829182798}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann2['extended'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {
    "height": "116px",
    "width": "277px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 424.0,
   "position": {
    "height": "40px",
    "left": "1018px",
    "right": "223px",
    "top": "13px",
    "width": "419px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
